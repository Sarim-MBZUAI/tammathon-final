{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from backbones import get_model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained model\n",
    "data = torch.load(\"/l/users/sarim.hashmi/Thesis/hackathon/PetFace/outputs/cat/arcface_with_cutout/model_last.pt\", map_location=torch.device('cpu'))\n",
    "\n",
    "backbone = get_model(\"r50\", dropout=0.0, fp16=True, num_features=768).cuda()\n",
    "backbone.load_state_dict(data[\"state_dict_backbone\"])\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_files, root_dir, transform=None, is_test=False):\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        raw_labels = []\n",
    "        for csv_file in csv_files:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            for _, row in tqdm(df.iterrows(), desc=f\"Loading {csv_file}\", total=len(df)):\n",
    "                img_path = os.path.join(root_dir, row['filename'])\n",
    "                label = row['label'] if not is_test else row['filename']  # dummy label for test\n",
    "                self.samples.append((img_path, label))\n",
    "                if not is_test:\n",
    "                    raw_labels.append(label)\n",
    "\n",
    "        if not is_test:\n",
    "            classes = sorted(set(raw_labels))\n",
    "            self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "            self.samples = [\n",
    "                (path, self.class_to_idx[label]) for path, label in self.samples\n",
    "            ]\n",
    "            self.classes = classes\n",
    "        else:\n",
    "            self.class_to_idx = {}\n",
    "            self.classes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "\n",
    "        # Fix Kaggle val path (could generalize if needed)\n",
    "        if \"/val/\" in path:\n",
    "            path = os.path.join(\"/l/users/sarim.hashmi/Thesis/hackathon/val/val\", *path.split(\"/val/\")[1:])\n",
    "\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def get_dataloaders(train_csv, val_csv, root_dir=\".\", transform=None,\n",
    "                    seed=42, train_ratio=0.8, batch_size=32, num_workers=4):\n",
    "    \"\"\"Returns: train_loader, val_loader, class_names\"\"\"\n",
    "    \n",
    "    print(\"Building dataset...\")\n",
    "    full_ds = CSVDataset(\n",
    "        csv_files=[train_csv, val_csv],\n",
    "        root_dir=root_dir,\n",
    "        transform=transform,\n",
    "        is_test=False,\n",
    "    )\n",
    "\n",
    "    train_size = int(train_ratio * len(full_ds))\n",
    "    val_size = len(full_ds) - train_size\n",
    "    \n",
    "    print(f\"Splitting dataset: {train_size} train, {val_size} validation samples\")\n",
    "    train_ds, val_ds = random_split(\n",
    "        full_ds, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, full_ds.classes\n",
    "\n",
    "\n",
    "def get_test_loader(test_csv, root_dir=\".\", transform=None, batch_size=32, num_workers=4):\n",
    "    \"\"\"Returns: test_loader\"\"\"\n",
    "    \n",
    "    print(\"Building test dataset...\")\n",
    "    test_ds = CSVDataset(\n",
    "        csv_files=[test_csv],\n",
    "        root_dir=root_dir,\n",
    "        transform=transform,\n",
    "        is_test=True\n",
    "    )\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def compute_class_means(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Computes the mean feature vector for each class in `dataloader`.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sum_feats = {}\n",
    "    cnt = {}\n",
    "\n",
    "    print(\"Computing class means...\")\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs)\n",
    "            feats = feats.cpu()\n",
    "            labels = labels.cpu()\n",
    "\n",
    "            for feat, lbl in zip(feats, labels):\n",
    "                lbl = int(lbl)\n",
    "                if lbl not in sum_feats:\n",
    "                    sum_feats[lbl] = feat.clone()\n",
    "                    cnt[lbl] = 1\n",
    "                else:\n",
    "                    sum_feats[lbl] += feat\n",
    "                    cnt[lbl] += 1\n",
    "\n",
    "    class_means = {\n",
    "        lbl: sum_feats[lbl] / cnt[lbl]\n",
    "        for lbl in sum_feats\n",
    "    }\n",
    "    return class_means\n",
    "\n",
    "\n",
    "def predict_with_ncm(model, dataloader, class_means, device, metric=\"euclidean\", topk=3):\n",
    "    \"\"\"\n",
    "    Predict labels on a dataset using Nearest Class Mean.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    top1_preds, topk_preds, targets = [], [], []\n",
    "\n",
    "    # Stack class means (C, D)\n",
    "    labels_sorted = sorted(class_means.keys())\n",
    "    mean_matrix = torch.stack([class_means[lbl] for lbl in labels_sorted])  # (C, D)\n",
    "    print(f\"Class mean matrix shape: {mean_matrix.size()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, target_label in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            img = img.to(device)\n",
    "\n",
    "            # Feature extraction\n",
    "            feats = model(img).cpu()  # (B, D)\n",
    "            \n",
    "            if metric == \"euclidean\":\n",
    "                dists = torch.cdist(feats, mean_matrix, p=2)  # (B, C)\n",
    "                topk_idx = torch.topk(dists, topk, largest=False).indices  # (B, k)\n",
    "            elif metric == \"cosine\":\n",
    "                # Normalize both\n",
    "                feats_norm = F.normalize(feats, dim=1)  # (B, D)\n",
    "                means_norm = F.normalize(mean_matrix, dim=1)  # (C, D)\n",
    "                sims = feats_norm @ means_norm.T  # (B, C)\n",
    "                topk_idx = torch.topk(sims, topk, largest=True).indices  # (B, k)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported metric\")\n",
    "\n",
    "            # Map indices to label values\n",
    "            for i in range(topk_idx.size(0)):\n",
    "                pred_labels = [labels_sorted[j] for j in topk_idx[i].tolist()]\n",
    "                top1_preds.append(pred_labels[0])\n",
    "                topk_preds.append(pred_labels)\n",
    "                targets.append(target_label[i])\n",
    "\n",
    "    return top1_preds, topk_preds, targets\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create dataloaders\n",
    "print(\"Setting up dataloaders...\")\n",
    "train_loader, val_loader, classes = get_dataloaders(\n",
    "    train_csv='/l/users/sarim.hashmi/Thesis/hackathon/train.csv',\n",
    "    val_csv='/l/users/sarim.hashmi/Thesis/hackathon/val.csv',\n",
    "    root_dir='/l/users/sarim.hashmi/Thesis/hackathon/PetFace/data/PetFace/images/',\n",
    "    transform=transform,\n",
    "    seed=123,\n",
    "    train_ratio=0.9,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "test_loader = get_test_loader(\n",
    "    test_csv='/l/users/sarim.hashmi/Thesis/hackathon/test/test.csv',\n",
    "    root_dir='/l/users/sarim.hashmi/Thesis/hackathon/test/',\n",
    "    transform=transform,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = backbone.to(device)\n",
    "\n",
    "# Compute class means\n",
    "class_means = compute_class_means(model, train_loader, device)\n",
    "\n",
    "# Display class means\n",
    "for lbl, mean_vec in list(class_means.items())[:5]:  # Show first 5 for brevity\n",
    "    print(f\"Class {lbl}: mean feature norm = {mean_vec.norm().item():.3f}\")\n",
    "\n",
    "# Save class means\n",
    "torch.save(class_means, \"class_means2.pt\")\n",
    "print(\"Saved class means to class_means2.pt\")\n",
    "\n",
    "# Load class means (you can comment this out if not needed)\n",
    "class_means = torch.load(\"class_means2.pt\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "pred, topk, target = predict_with_ncm(backbone, test_loader, class_means, device, metric=\"cosine\")\n",
    "\n",
    "# Create output dataframe\n",
    "print(\"Saving predictions...\")\n",
    "df = pd.DataFrame(topk, columns=[\"label_1\", \"label_2\", \"label_3\"])\n",
    "df.insert(0, \"filename\", target)  # insert filenames as first column\n",
    "\n",
    "# Save to CSV with index\n",
    "df.index.name = \"#\"\n",
    "df.to_csv(\"top3_predictions_new.csv\", index=True)\n",
    "print(\"Done! Predictions saved to top3_predictions_new.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
